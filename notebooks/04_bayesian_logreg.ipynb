{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "44aba87a",
      "metadata": {},
      "source": [
        "# 04 — Bayesian Logistic Regression (with Uncertainty)\n",
        "\n",
        "This notebook fits a **Bayesian logistic regression** to predict ore-occurrence likelihood on a geospatial grid and **quantifies uncertainty**.\n",
        "\n",
        "**Why these steps matter**\n",
        "- **Drop constant/near-constant columns**: avoids singular directions that break the sampler on sparse, one-hot features.\n",
        "- **Standardize features**: keeps coefficients on comparable scales (especially with mixed one-hot + continuous features like gravity).\n",
        "- **Intercept prior = base rate**: set the prior mean for the intercept to `logit(prevalence)` so the model starts near realistic class odds in class-imbalanced data.\n",
        "- **`logit_p=` in Bernoulli**: avoids numerical overflow of `sigmoid` at extreme logits.\n",
        "- **Robust NUTS init (`jitter+adapt_diag`) + higher target_accept**: safer step-size and mass-matrix adaptation on real-world, imbalanced datasets.\n",
        "\n",
        "**Outputs**\n",
        "- `data/processed/mean_probs.npy` — posterior mean probability per grid cell  \n",
        "- `data/processed/std_probs.npy` — posterior std-dev (uncertainty) per grid cell\n",
        "\n",
        "These files power the Streamlit viewer and figure exporters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d75a3dfb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "## 1) Install deps\n",
        "%pip install -q -r ../requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e8c6e060",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Including geology features: 18 columns\n",
            "Including gravity feature\n",
            "Including gravity gradient\n",
            "Including geochem: 8 columns\n",
            "Including magnetics: 3 cols\n",
            "Feature matrix: (7646, 33) | Labels: (7646,)\n"
          ]
        }
      ],
      "source": [
        "## 2) Imports & data loading\n",
        "import os\n",
        "import numpy as np\n",
        "import joblib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pymc as pm\n",
        "import arviz as az\n",
        "from scipy.special import logit\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "project_root = Path.cwd().parent\n",
        "\n",
        "# Required inputs\n",
        "Xc = np.load(project_root / \"data/processed/X_coords.npy\")       # (n, 2) or similar\n",
        "y  = np.load(project_root / \"data/processed/y_labels.npy\")       # (n,)\n",
        "grid = joblib.load(project_root / \"data/processed/grid_gdf.joblib\")  # GeoDataFrame of grid cells\n",
        "\n",
        "# Optional inputs\n",
        "parts = [Xc]\n",
        "\n",
        "# Geology one-hots / encodings (optional)\n",
        "geo_path = project_root / \"data/processed/X_geo.npy\"\n",
        "if os.path.exists(geo_path):\n",
        "    Xg = np.load(geo_path)\n",
        "    if Xg.shape[0] == Xc.shape[0]:\n",
        "        parts.append(Xg)\n",
        "        print(f\"Including geology features: {Xg.shape[1]} columns\")\n",
        "    else:\n",
        "        print(\"Found X_geo.npy but row count mismatches X_coords; skipping geology.\")\n",
        "\n",
        "# Gravity (optional, continuous)\n",
        "grav_path = project_root / \"data/processed/X_gravity.npy\"\n",
        "if os.path.exists(grav_path):\n",
        "    Xgrav = np.load(grav_path).reshape(-1, 1)\n",
        "    if Xgrav.shape[0] == Xc.shape[0]:\n",
        "        parts.append(Xgrav)\n",
        "        print(\"Including gravity feature\")\n",
        "    else:\n",
        "        print(\"Found X_gravity.npy but row count mismatches X_coords; skipping gravity.\")\n",
        "\n",
        "# Optional gravity gradient\n",
        "grad_path = project_root / 'data/processed/X_gravity_grad.npy'\n",
        "if os.path.exists(grad_path):\n",
        "    Xgg = np.load(grad_path).reshape(-1, 1)\n",
        "    if Xgg.shape[0] == Xc.shape[0]:\n",
        "        parts.append(Xgg)\n",
        "        print(\"Including gravity gradient\")\n",
        "        \n",
        "# Optional geochemistry\n",
        "geochem_path = project_root / \"data/processed/X_geochem.npy\"\n",
        "if os.path.exists(geochem_path):\n",
        "    Xgc = np.load(geochem_path)\n",
        "    if Xgc.shape[0] == Xc.shape[0]:\n",
        "        parts.append(Xgc)\n",
        "        print(f\"Including geochem: {Xgc.shape[1]} columns\")\n",
        "        \n",
        "# Optional magnetic\n",
        "if os.path.exists(project_root / \"data/processed/X_mag.npy\"):\n",
        "    Xmag = np.load(project_root / \"data/processed/X_mag.npy\")\n",
        "    if Xmag.shape[0] == Xc.shape[0]:   # or Xc.shape[0] if you build baseline first\n",
        "        parts.append(Xmag)\n",
        "        print(f\"Including magnetics: {Xmag.shape[1]} cols\")\n",
        "        \n",
        "# Final feature matrix\n",
        "X = np.hstack(parts)\n",
        "print(\"Feature matrix:\", X.shape, \"| Labels:\", y.shape)\n",
        "\n",
        "# Sanity checks\n",
        "assert X.shape[0] == y.shape[0], \"Row count mismatch between X and y\"\n",
        "assert set(np.unique(y)).issubset({0,1}), \"y must be binary {0,1}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5b8f0575",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed 2 near-constant columns for stability\n",
            "Class prevalence p0=0.0871 -> intercept prior mean logit(p0)=-2.350\n"
          ]
        }
      ],
      "source": [
        "## 3) Preprocess for stability (drop constants, standardize)\n",
        "# Remove constant / near-constant columns (important with one-hots)\n",
        "std = X.std(axis=0, ddof=0)\n",
        "keep = std > 1e-8\n",
        "removed = int((~keep).sum())\n",
        "if removed:\n",
        "    print(f\"Removed {removed} near-constant columns for stability\")\n",
        "\n",
        "Xb = X[:, keep]\n",
        "\n",
        "# Standardize all remaining columns\n",
        "mu = Xb.mean(axis=0)\n",
        "sd = Xb.std(axis=0, ddof=0)\n",
        "Xb = (Xb - mu) / sd\n",
        "\n",
        "# Intercept prior at base rate (class prevalence)\n",
        "p0 = float(np.clip(y.mean(), 1e-4, 1-1e-4))\n",
        "mu_intercept = logit(p0)\n",
        "print(f\"Class prevalence p0={p0:.4f} -> intercept prior mean logit(p0)={mu_intercept:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a355195e",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing NUTS using jitter+adapt_diag...\n",
            "Multiprocess sampling (4 chains in 4 jobs)\n",
            "NUTS: [beta, intercept]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e26b444d7c1747ffb4bb851a4cfd5779",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Sampling 4 chains for 1_500 tune and 1_000 draw iterations (6_000 + 4_000 draws total) took 155 seconds.\n"
          ]
        }
      ],
      "source": [
        "## 4) Fit Bayesian logistic regression (PyMC, robust settings)\n",
        "with pm.Model() as model:\n",
        "    # Coefficients: moderately tight prior stabilizes with mixed feature scales\n",
        "    beta = pm.Normal(\"beta\", mu=0.0, sigma=0.5, shape=Xb.shape[1])\n",
        "    # Intercept centered at base rate\n",
        "    intercept = pm.Normal(\"intercept\", mu=mu_intercept, sigma=2.0)\n",
        "\n",
        "    logits = intercept + pm.math.dot(Xb, beta)\n",
        "\n",
        "    # Use logit_p to avoid sigmoid overflow/underflow\n",
        "    y_obs = pm.Bernoulli(\"y_obs\", logit_p=logits, observed=y)\n",
        "\n",
        "    trace = pm.sample(\n",
        "        draws=1000,\n",
        "        tune=1500,\n",
        "        chains=4,\n",
        "        cores=4,\n",
        "        init=\"jitter+adapt_diag\",\n",
        "        target_accept=0.95,\n",
        "        random_seed=42,\n",
        "        progressbar=True,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3bcc87cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bayesian ROC AUC: 0.680\n"
          ]
        }
      ],
      "source": [
        "## 5) Posterior predictive probabilities (mean & uncertainty) and AUC\n",
        "# Extract posterior samples\n",
        "beta_s = trace.posterior[\"beta\"].values        # (chains, draws, n_features)\n",
        "int_s  = trace.posterior[\"intercept\"].values   # (chains, draws)\n",
        "\n",
        "# logits: (chains, draws, n_obs) = (c,d,f) · (f,n) + intercept\n",
        "logits = int_s[..., None] + np.tensordot(beta_s, Xb.T, axes=([2],[0]))\n",
        "post_p = 1.0 / (1.0 + np.exp(-logits))  # sigmoid\n",
        "\n",
        "mean_probs = post_p.mean(axis=(0,1))  # (n_obs,)\n",
        "std_probs  = post_p.std(axis=(0,1))   # (n_obs,)\n",
        "\n",
        "# Quick diagnostic\n",
        "auc_bayes = roc_auc_score(y, mean_probs)\n",
        "print(f\"Bayesian ROC AUC: {auc_bayes:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4a28a200",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved data/processed/mean_probs.npy and std_probs.npy\n"
          ]
        }
      ],
      "source": [
        "## 6) Save arrays for app/figures\n",
        "os.makedirs(project_root / \"data/processed\", exist_ok=True)\n",
        "np.save(project_root / \"data/processed/mean_probs.npy\", mean_probs)\n",
        "np.save(project_root / \"data/processed/std_probs.npy\",  std_probs)\n",
        "print(\"Saved data/processed/mean_probs.npy and std_probs.npy\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
